\section{Mixture of Experts (MoE)}
\label{sec:moe}

Mixture of Experts（MoE）是一种稀疏激活架构，通过让每个token只激活部分参数，实现了``大模型容量、小模型计算量''的目标。本节重点介绍DeepSeek MoE架构及其在工业界的广泛应用。

\subsection{MoE基础}

\subsubsection{核心思想}

传统Dense模型中，每个token都要经过所有参数。MoE的核心思想是：用\textbf{路由器}（Router）为每个token选择最相关的\textbf{专家}（Expert），只激活部分参数：
\begin{equation}
    y = \sum_{i=1}^{N} g_i(x) \cdot E_i(x)
\end{equation}
其中$E_i$是第$i$个专家（通常是FFN），$g_i(x)$是路由器为token $x$分配给专家$i$的权重。

\subsubsection{Top-K路由}

标准的Top-K路由机制：
\begin{align}
    s_i &= x \cdot W_r^{(i)} \quad \text{(路由分数)} \\
    g_i &= \begin{cases}
        \text{softmax}(s)_i & \text{if } i \in \text{Top-}K(s) \\
        0 & \text{otherwise}
    \end{cases}
\end{align}
其中$W_r$是路由器的可学习参数。每个token只被发送到$K$个得分最高的专家。

\begin{table}[htbp]
\centering
\caption{MoE术语对照}
\label{tab:moe_terms}
\begin{tabular}{ll}
\toprule
术语 & 含义 \\
\midrule
总参数量 & 模型所有参数（包括所有专家） \\
激活参数量 & 单个token前向传播使用的参数 \\
专家数$N$ & 可选专家的总数 \\
激活专家数$K$ & 每个token选择的专家数 \\
稀疏度 & $N/K$，越大表示越稀疏 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{DeepSeek MoE架构}

DeepSeek~\citep{dai2024deepseekmoe}提出了目前最具影响力的MoE设计，被DeepSeek-V2~\citep{deepseek2024v2}、DeepSeek-V3~\citep{deepseek2024v3}等模型采用。

\subsubsection{Fine-grained Expert Segmentation}

传统MoE使用少量大专家（如8个），DeepSeek提出\textbf{细粒度专家分割}：将专家数量增加$m$倍，同时将每个专家的参数减少$m$倍：
\begin{equation}
    N \to mN, \quad K \to mK, \quad \text{Expert Size} \to \frac{1}{m}
\end{equation}

\textbf{优势}：更多专家组合提供更灵活的知识表示。例如，从8个专家选2个有$\binom{8}{2} = 28$种组合，而从64个专家选16个有$\binom{64}{16} \approx 4.9 \times 10^{14}$种组合。

\subsubsection{Shared Expert Isolation}

除了路由专家外，DeepSeek引入\textbf{共享专家}（Shared Experts）：
\begin{equation}
    y = \underbrace{\sum_{i=1}^{K_s} E_i^{shared}(x)}_{\text{共享专家}} + \underbrace{\sum_{j=1}^{K_r} g_j(x) \cdot E_j^{routed}(x)}_{\text{路由专家}}
\end{equation}

\textbf{设计哲学}：
\begin{itemize}
    \item \textbf{共享专家}：捕获所有token都需要的通用知识（如语法、常识）
    \item \textbf{路由专家}：捕获领域特定知识（如数学、代码、医学）
\end{itemize}

这种分离减少了路由专家之间的知识冗余，提高了专业化程度。

\subsubsection{DeepSeek-V2/V3配置}

\begin{table}[htbp]
\centering
\caption{DeepSeek MoE模型配置}
\label{tab:deepseek_moe_config}
\begin{tabular}{lccc}
\toprule
模型 & 总参数 & 激活参数 & 专家配置 \\
\midrule
DeepSeek-V2 & 236B & 21B & 160路由 + 2共享 \\
DeepSeek-V3 & 671B & 37B & 256路由 + 1共享 \\
DeepSeek-R1 & 671B & 37B & 同V3 \\
\bottomrule
\end{tabular}
\end{table}

DeepSeek-V3每个token激活8个路由专家 + 1个共享专家，稀疏度高达$256/8 = 32$。

\subsection{负载均衡策略}

MoE训练的核心挑战是\textbf{负载均衡}：如果某些专家被过度选择，会导致：
\begin{enumerate}
    \item \textbf{路由崩塌}（Routing Collapse）：所有token都选同几个专家
    \item \textbf{计算效率下降}：专家并行时负载不均
    \item \textbf{知识浪费}：部分专家从未被训练
\end{enumerate}

\subsubsection{传统方法：辅助损失}

早期方法（如Switch Transformer~\citep{fedus2022switch}）使用辅助损失强制负载均衡：
\begin{equation}
    \mathcal{L}_{aux} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i
\end{equation}
其中$f_i$是专家$i$实际处理的token比例，$P_i$是路由分数的平均值。

\textbf{问题}：辅助损失与主任务损失相互竞争，$\alpha$过大会损害模型性能。

\subsubsection{DeepSeek-V2：多级辅助损失}

DeepSeek-V2引入三级辅助损失~\citep{deepseek2024v2}：
\begin{enumerate}
    \item \textbf{Expert-level}：平衡单个专家的负载
    \item \textbf{Device-level}：平衡不同设备上的专家负载
    \item \textbf{Communication-level}：减少跨设备通信
\end{enumerate}

\subsubsection{DeepSeek-V3：无辅助损失负载均衡}

DeepSeek-V3提出革命性的\textbf{Auxiliary-Loss-Free负载均衡}~\citep{deepseek2024v3}：

\textbf{核心思想}：为每个专家引入可调偏置项$b_i$，仅用于路由决策，不参与loss计算：
\begin{equation}
    s_i' = s_i + b_i
\end{equation}

\textbf{动态调整}：
\begin{itemize}
    \item 专家过载时，减小$b_i$降低被选概率
    \item 专家空闲时，增大$b_i$提高被选概率
\end{itemize}

\textbf{关键优势}：负载均衡目标与质量优化目标完全解耦，不再相互竞争。实验表明V3在整个训练过程中保持良好的负载均衡，无需丢弃任何token。

\subsection{路由约束与通信优化}

\subsubsection{Node-Limited Routing}

在分布式训练中，专家分布在不同节点上。跨节点通信成本高昂。DeepSeek引入\textbf{节点限制路由}：
\begin{quote}
    每个token最多被发送到$M$个节点。
\end{quote}

这限制了All-to-All通信的范围，大幅减少通信开销。

\subsubsection{Expert Tensor Parallelism}

MiniMax提出\textbf{Expert Tensor Parallel}（ETP）：将单个专家的参数切分到多个设备，而非将不同专家放在不同设备。这种方式更适合细粒度专家架构。

\subsection{工业应用对比}

\begin{table}[htbp]
\centering
\caption{主流MoE模型对比}
\label{tab:moe_comparison}
\begin{tabular}{lcccc}
\toprule
模型 & 总参数 & 激活参数 & 专家配置 & 特点 \\
\midrule
DeepSeek-V3 & 671B & 37B & 256+1 & 无辅助损失均衡 \\
MiniMax-01 & 456B & 45.9B & 32专家 & Lightning Attention \\
Kimi K2 & 1T & 32B & 384路由 & MuonClip优化器 \\
Qwen2-57B-A14B & 57B & 14B & 60+4共享 & Upcycling \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{DeepSeek-V3}

\textbf{关键创新}：
\begin{itemize}
    \item Auxiliary-Loss-Free负载均衡
    \item Multi-Token Prediction (MTP)
    \item FP8混合精度训练
    \item 仅需2.788M H800 GPU小时完成训练
\end{itemize}

\textbf{性能}：671B参数，每token激活37B，在多项benchmark上达到GPT-4级别。

\subsubsection{MiniMax-01}

\textbf{架构特点}：
\begin{itemize}
    \item 32个专家，每token激活约45.9B参数
    \item 结合Lightning Attention的混合架构
    \item 每7层线性注意力后接1层Softmax注意力
\end{itemize}

\textbf{长上下文}：训练1M tokens，推理可扩展至4M tokens。

\subsubsection{Kimi K2}

\textbf{规模}：1T总参数，32B激活参数——目前最大的开源MoE模型之一。

\textbf{架构}：
\begin{itemize}
    \item 类似DeepSeek-V3的MLA + MoE架构
    \item 384个路由专家，每token激活8个
    \item 稀疏度：$384/8 = 48$（高于DeepSeek-V3的32）
\end{itemize}

\textbf{训练}：使用Muon优化器（MuonClip变体），15.5T tokens训练，零训练不稳定性。

\subsubsection{Qwen MoE}

Qwen采用\textbf{Upcycling}策略：从Dense模型初始化MoE专家。

\textbf{Qwen2-57B-A14B}：
\begin{itemize}
    \item 从Qwen2-7B upcycle而来
    \item 60个路由专家 + 4个共享专家
    \item 激活14B参数，性能接近34B Dense模型
\end{itemize}

\subsection{MoE的理论理解}

\subsubsection{稀疏性与容量}

MoE的核心trade-off是\textbf{稀疏性}与\textbf{模型容量}：
\begin{itemize}
    \item 更多专家 → 更大容量，但通信开销增加
    \item 更少激活专家 → 更高效率，但可能欠拟合
\end{itemize}

DeepSeek-V3的经验：256个专家 + 8个激活是一个good balance。

\subsubsection{专家专业化}

理想情况下，不同专家应学会处理不同类型的知识：
\begin{itemize}
    \item 某些专家处理数学推理
    \item 某些专家处理代码生成
    \item 某些专家处理多语言
\end{itemize}

共享专家的引入帮助路由专家更好地专业化，避免``每个专家都学一点通用知识''。

\begin{remark}[MoE vs Dense的选择]
MoE并非总是优于Dense：
\begin{itemize}
    \item \textbf{MoE优势}：相同计算预算下更大容量；推理时更高效
    \item \textbf{Dense优势}：更简单的训练和部署；在某些任务上更稳定
\end{itemize}
当前趋势是在超大规模模型（100B+）中使用MoE，中小规模仍以Dense为主。
\end{remark}

\subsection{MoE训练技巧}

\paragraph{负载均衡}
\begin{itemize}
    \item 优先使用DeepSeek-V3的无辅助损失方法
    \item 如使用辅助损失，系数$\alpha$需仔细调优（通常$0.01 \sim 0.1$）
\end{itemize}

\paragraph{专家并行}
\begin{itemize}
    \item 小规模：所有专家放单卡
    \item 中规模：Expert Parallelism，不同专家在不同卡
    \item 大规模：结合TP、EP、PP的混合并行
\end{itemize}

\paragraph{Upcycling}
从Dense模型初始化MoE可以加速收敛：
\begin{enumerate}
    \item 复制Dense模型的FFN作为各专家的初始化
    \item 随机初始化路由器
    \item 继续预训练，专家逐渐分化
\end{enumerate}

\paragraph{容量因子}
传统MoE设置容量因子（Capacity Factor）限制每个专家处理的最大token数，超出的token被丢弃。DeepSeek-V3证明：良好的负载均衡策略可以\textbf{完全避免token丢弃}。
