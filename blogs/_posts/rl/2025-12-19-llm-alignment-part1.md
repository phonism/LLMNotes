---
layout: post
title: "RL å­¦ä¹ ç¬”è®°ï¼ˆäº”ï¼‰ï¼šRLHF ä¸ DPO"
date: 2025-12-19 07:00:00
author: Phonism
tags: [RL, LLM, RLHF, DPO, Alignment]
lang: zh
translation: /en/llm-alignment-part1/
---

æœ¬æ–‡æ˜¯å¼ºåŒ–å­¦ä¹ ç³»åˆ—çš„ç¬¬äº”ç¯‡ï¼Œå¼€å§‹è¿›å…¥ LLM ä¸ RL ç»“åˆçš„é¢†åŸŸã€‚æœ¬ç¯‡ä»‹ç» LLM å¯¹é½çš„ RL å»ºæ¨¡ã€ç»å…¸çš„ RLHF ä¸‰é˜¶æ®µæ–¹æ³•ï¼Œä»¥åŠæ›´ç®€æ´çš„ DPO æ–¹æ³•ã€‚

## å¼•è¨€ï¼šä»é¢„è®­ç»ƒåˆ°å¯¹é½

### æ ¸å¿ƒé—®é¢˜

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æµ·é‡æ–‡æœ¬é¢„è®­ç»ƒï¼Œè·å¾—äº†å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ä½†é¢„è®­ç»ƒç›®æ ‡ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼‰ä¸äººç±»æœŸæœ›çš„è¡Œä¸ºä¹‹é—´å­˜åœ¨é¸¿æ²Ÿï¼š

> **é¢„è®­ç»ƒçš„ LLM åªå­¦ä¼šäº†"åƒäººç±»ä¸€æ ·è¯´è¯"ï¼Œä½†æ²¡æœ‰å­¦ä¼š"æŒ‰äººç±»æœŸæœ›è¡Œäº‹"ã€‚**
>
> å¦‚ä½•è®© LLM ä¸ä»…æµåˆ©ï¼Œè¿˜èƒ½æœ‰å¸®åŠ©ã€è¯šå®ã€æ— å®³ï¼Ÿ

è¿™å°±æ˜¯ **LLM å¯¹é½**ï¼ˆAlignmentï¼‰é—®é¢˜ã€‚è€Œå¼ºåŒ–å­¦ä¹ æ­£æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦ RLï¼Ÿ

ç›‘ç£å­¦ä¹ ï¼ˆSFTï¼‰å¯ä»¥è®©æ¨¡å‹æ¨¡ä»¿é«˜è´¨é‡å›å¤ï¼Œä½†å­˜åœ¨å±€é™ï¼š

1. **åˆ†å¸ƒå—é™**ï¼šåªèƒ½å­¦ä¹ è®­ç»ƒé›†ä¸­å‡ºç°çš„å›å¤æ–¹å¼
2. **æ— æ³•è¡¨è¾¾åå¥½**ï¼šéš¾ä»¥åŒºåˆ†"å¥½"å’Œ"æ›´å¥½"
3. **æ— æ³•æ¢ç´¢**ï¼šä¸ä¼šå°è¯•æ–°çš„å›ç­”ç­–ç•¥

å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸åŒçš„è§†è§’ï¼š
- å°† LLM ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸º MDP
- ç”¨äººç±»åå¥½å®šä¹‰å¥–åŠ±å‡½æ•°
- é€šè¿‡æœ€å¤§åŒ–å¥–åŠ±æ¥ä¼˜åŒ–ç­–ç•¥

<div class="tikz-container">
<script type="text/tikz">
\begin{tikzpicture}[
    box/.style={draw, rounded corners, minimum width=3cm, minimum height=1cm, align=center},
    arrow/.style={->, thick, >=stealth}
]
    % é¢„è®­ç»ƒ
    \node[box, fill=blue!20] (pt) at (0, 0) {é¢„è®­ç»ƒ\\ï¼ˆNext Token Predictionï¼‰};

    % SFT
    \node[box, fill=green!20] (sft) at (5, 0) {ç›‘ç£å¾®è°ƒ SFT\\ï¼ˆæ¨¡ä»¿é«˜è´¨é‡å›å¤ï¼‰};

    % RLHF
    \node[box, fill=orange!20] (rlhf) at (10, 0) {RL å¯¹é½\\ï¼ˆä¼˜åŒ–äººç±»åå¥½ï¼‰};

    % ç®­å¤´
    \draw[arrow] (pt) -- node[above, font=\small, yshift=5pt] {è¯­è¨€èƒ½åŠ›} (sft);
    \draw[arrow] (sft) -- node[above, font=\small, yshift=5pt] {æŒ‡ä»¤éµå¾ª} (rlhf);

    % æ ‡æ³¨
    \node[font=\scriptsize, gray] at (0, -1) {ä¼šè¯´è¯};
    \node[font=\scriptsize, gray] at (5, -1) {èƒ½å›ç­”é—®é¢˜};
    \node[font=\scriptsize, gray] at (10, -1) {æŒ‰äººç±»æœŸæœ›è¡Œäº‹};
\end{tikzpicture}
</script>
</div>

## LLM å¯¹é½çš„ RL å»ºæ¨¡

### State/Action/Reward å®šä¹‰

å°† LLM å¯¹é½é—®é¢˜å»ºæ¨¡ä¸º RL é—®é¢˜ï¼š

> **LLM çš„ RL å»ºæ¨¡**
> - **State** $s_t$ï¼šprompt $x$ + å·²ç”Ÿæˆçš„ token åºåˆ— $y_{<t} = (y_1, \ldots, y_{t-1})$
> - **Action** $a_t$ï¼šä¸‹ä¸€ä¸ª token $y_t$ï¼ˆè¯è¡¨å¤§å° $\|\mathcal{V}\| \sim$ 100kï¼‰
> - **Policy** $\pi_\theta(a\|s)$ï¼šLLM æœ¬èº«ï¼Œ$\pi_\theta(y_t \| x, y_{<t})$
> - **Trajectory** $\tau$ï¼šå®Œæ•´çš„ç”Ÿæˆåºåˆ— $y = (y_1, y_2, \ldots, y_T)$
> - **Reward** $r$ï¼šé€šå¸¸åªåœ¨åºåˆ—ç»“æŸæ—¶ç»™å‡º

<div class="tikz-container">
<script type="text/tikz">
\begin{tikzpicture}[
    state/.style={draw, rounded corners, fill=blue!15, minimum width=2.5cm, minimum height=0.8cm, align=center, font=\small},
    action/.style={circle, draw, fill=orange!20, minimum size=0.6cm, font=\scriptsize},
    arrow/.style={->, thick, >=stealth}
]
    % çŠ¶æ€åºåˆ—
    \node[state] (s0) at (0, 0) {$x$ (prompt)};
    \node[state] (s1) at (3.5, 0) {$x, y_1$};
    \node[state] (s2) at (7, 0) {$x, y_1, y_2$};
    \node[font=\small] at (9.5, 0) {$\cdots$};
    \node[state] (sT) at (12, 0) {$x, y_{1:T}$};

    % åŠ¨ä½œ
    \node[action] (a1) at (1.75, 1) {$y_1$};
    \node[action] (a2) at (5.25, 1) {$y_2$};
    \node[action] (aT) at (10.5, 1) {$y_T$};

    % å¥–åŠ±
    \node[font=\small, red] at (13.5, 0) {$r(x, y)$};

    % è¿æ¥
    \draw[arrow] (s0) -- (a1);
    \draw[arrow] (a1) -- (s1);
    \draw[arrow] (s1) -- (a2);
    \draw[arrow] (a2) -- (s2);
    \draw[arrow, dashed] (s2) -- (9, 0);
    \draw[arrow] (10, 0) -- (aT);
    \draw[arrow] (aT) -- (sT);
    \draw[arrow, red] (sT) -- (13.2, 0);

    % æ ‡æ³¨
    \node[font=\scriptsize, gray] at (1.75, 1.6) {$\pi_\theta(y_1|x)$};
    \node[font=\scriptsize, gray] at (5.25, 1.6) {$\pi_\theta(y_2|x,y_1)$};
\end{tikzpicture}
</script>
</div>

LLM RL çš„ç‰¹ç‚¹ï¼š
- **åŠ¨ä½œç©ºé—´å·¨å¤§**ï¼šè¯è¡¨é€šå¸¸æœ‰ 10 ä¸‡+ token
- **ç¡®å®šæ€§çŠ¶æ€è½¬ç§»**ï¼šä¸‹ä¸€çŠ¶æ€ = å½“å‰çŠ¶æ€ + æ–° token
- **Episode = ä¸€æ¬¡å®Œæ•´ç”Ÿæˆ**ï¼šä» prompt åˆ° EOS
- **ç¨€ç–å¥–åŠ±**ï¼šåªæœ‰åºåˆ—ç»“æŸæ—¶æ‰æœ‰å¥–åŠ±ä¿¡å·

### ç¨€ç–å¥–åŠ±é—®é¢˜

LLM å¯¹é½çš„å…¸å‹å¥–åŠ±ç»“æ„ï¼š

$$r_t = \begin{cases} 0 & t < T \\ r_\phi(x, y) & t = T \text{ï¼ˆåºåˆ—ç»“æŸï¼‰} \end{cases}$$

ç¨€ç–å¥–åŠ±å¸¦æ¥çš„æŒ‘æˆ˜ï¼š
- **ä¿¡ç”¨åˆ†é…å›°éš¾**ï¼šæœ€ç»ˆå¥–åŠ±å¦‚ä½•å½’å› åˆ°æ¯ä¸ª tokenï¼Ÿ
- **æ¢¯åº¦ä¿¡å·å¼±**ï¼šå¤§éƒ¨åˆ†æ—¶åˆ»æ²¡æœ‰å­¦ä¹ ä¿¡å·
- **é•¿åºåˆ—å°¤å…¶å›°éš¾**ï¼šä¿¡å·éœ€è¦ä¼ æ’­å¾ˆè¿œï¼ˆæ•°åƒ tokenï¼‰

è§£å†³ç¨€ç–å¥–åŠ±çš„ä¸¤ç§æ€è·¯ï¼š
1. **åºåˆ—çº§æ–¹æ³•**ï¼šæŠŠæ•´ä¸ªåºåˆ—å½“ä½œä¸€ä¸ª banditï¼Œç”¨åºåˆ—å¥–åŠ±ç›´æ¥æ›´æ–°ï¼ˆå¦‚ REINFORCEï¼‰
2. **è¿‡ç¨‹å¥–åŠ±**ï¼šè®­ç»ƒ PRM æä¾›ä¸­é—´æ­¥éª¤çš„å¥–åŠ±ä¿¡å·

## RLHF ä¸‰é˜¶æ®µ

RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰æ˜¯ LLM å¯¹é½çš„ç»å…¸æ–¹æ³•ï¼Œç”± OpenAI åœ¨ InstructGPT ä¸­ç³»ç»ŸåŒ–ã€‚

### RLHF æ•´ä½“æ¶æ„

<div class="tikz-container">
<script type="text/tikz">
\begin{tikzpicture}[scale=0.9, every node/.style={scale=0.9},
    box/.style={draw, rounded corners, minimum width=2.8cm, minimum height=1cm, align=center},
    data/.style={draw, rounded corners, fill=gray!15, minimum width=2cm, minimum height=0.8cm, align=center, font=\small},
    arrow/.style={->, thick, >=stealth}
]
    % Stage 1
    \begin{scope}[shift={(-5, 0)}]
        \node[box, fill=blue!20] (pt) at (0, 2) {é¢„è®­ç»ƒæ¨¡å‹};
        \node[data] (sft_data) at (0, 0) {é«˜è´¨é‡å¯¹è¯\\æ•°æ®};
        \node[box, fill=green!20] (sft) at (0, -2) {SFT æ¨¡å‹\\$\pi_{\text{ref}}$};

        \draw[arrow] (pt) -- (sft);
        \draw[arrow] (sft_data) -- (sft);

        \node[font=\bfseries] at (0, 3.5) {Stage 1: SFT};
    \end{scope}

    % Stage 2
    \begin{scope}[shift={(0, 0)}]
        \node[box, fill=green!15] (sft2) at (0, 2) {SFT æ¨¡å‹};
        \node[data] (pref_data) at (0, 0) {äººç±»åå¥½æ•°æ®\\$(x, y_w, y_l)$};
        \node[box, fill=orange!20] (rm) at (0, -2) {Reward Model\\$r_\phi(x, y)$};

        \draw[arrow] (sft2) -- (rm);
        \draw[arrow] (pref_data) -- (rm);

        \node[font=\bfseries] at (0, 3.5) {Stage 2: RM};
    \end{scope}

    % Stage 3
    \begin{scope}[shift={(5.5, 0)}]
        \node[box, fill=green!15] (ref) at (-1.8, 2) {$\pi_{\text{ref}}$};
        \node[box, fill=orange!15] (rm2) at (1.8, 2) {$r_\phi$};
        \node[box, fill=purple!20] (ppo) at (0, 0) {PPO è®­ç»ƒ};
        \node[box, fill=red!20] (final) at (0, -2) {å¯¹é½æ¨¡å‹\\$\pi_\theta$};

        \draw[arrow] (ref) -- (ppo);
        \draw[arrow] (rm2) -- (ppo);
        \draw[arrow] (ppo) -- (final);

        \node[font=\bfseries] at (0, 3.5) {Stage 3: PPO};
    \end{scope}

    % è¿æ¥ç®­å¤´
    \draw[arrow, dashed, gray] (-3, -2) -- (-2, 2);
    \draw[arrow, dashed, gray] (2, -2) -- (4, 2);
\end{tikzpicture}
</script>
</div>

### Stage 1: Supervised Fine-Tuning (SFT)

ç”¨é«˜è´¨é‡å¯¹è¯æ•°æ®å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼š

$$L_{\text{SFT}}(\theta) = -\mathbb{E}_{(x,y) \sim \mathcal{D}_{\text{SFT}}} \left[ \log \pi_\theta(y|x) \right] = -\mathbb{E} \left[ \sum_{t=1}^{T} \log \pi_\theta(y_t | x, y_{<t}) \right]$$

SFT çš„ä½œç”¨ï¼š
- è®©æ¨¡å‹å­¦ä¼š"æŒ‡ä»¤éµå¾ª"çš„åŸºæœ¬æ ¼å¼
- æä¾› RL çš„èµ·ç‚¹ï¼ˆå‚è€ƒæ¨¡å‹ $\pi_{\text{ref}}$ï¼‰
- è¿‡æ»¤é¢„è®­ç»ƒä¸­çš„ä½è´¨é‡æ¨¡å¼

### Stage 2: Reward Model è®­ç»ƒ

ä»äººç±»åå¥½æ•°æ®ä¸­å­¦ä¹  Reward Modelã€‚

> **åå¥½æ•°æ®**ï¼šå¯¹äº prompt $x$ï¼Œäººç±»æ ‡æ³¨è€…æ¯”è¾ƒä¸¤ä¸ªå›å¤ï¼Œç»™å‡ºåå¥½ï¼š$y_w \succ y_l$ï¼ˆ$y_w$ ä¼˜äº $y_l$ï¼‰ã€‚

#### Bradley-Terry æ¨¡å‹

> **Bradley-Terry æ¨¡å‹**
>
> å‡è®¾äººç±»åå¥½éµå¾ª Bradley-Terry æ¨¡å‹â€”â€”åå¥½æ¦‚ç‡ç”±"èƒ½åŠ›å·®"å†³å®šï¼š
>
> $$P(y_w \succ y_l | x) = \sigma(r(x, y_w) - r(x, y_l)) = \frac{1}{1 + e^{-(r(x, y_w) - r(x, y_l))}}$$
>
> å…¶ä¸­ $\sigma(z) = \frac{1}{1+e^{-z}}$ æ˜¯ sigmoid å‡½æ•°ï¼Œ$r(x, y)$ æ˜¯å›å¤çš„"å¾—åˆ†"ã€‚

Bradley-Terry æ¨¡å‹çš„ç›´è§‰ï¼š
- å¥–åŠ±å·® = 0 æ—¶ï¼Œåå¥½æ¦‚ç‡ = 0.5ï¼ˆæ— æ³•åŒºåˆ†ï¼‰
- å¥–åŠ±å·®è¶Šå¤§ï¼Œåå¥½æ¦‚ç‡è¶Šæ¥è¿‘ 1ï¼ˆæ›´ç¡®å®šï¼‰
- æ¨¡å‹å‡è®¾åå¥½æ˜¯åŸºäº"å†…åœ¨è´¨é‡åˆ†æ•°"çš„æ¦‚ç‡æ¯”è¾ƒ

#### Reward Model è®­ç»ƒ

Reward Model çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–åå¥½æ•°æ®çš„ä¼¼ç„¶ï¼š

$$L_{\text{RM}}(\phi) = -\mathbb{E}_{(x, y_w, y_l)} \left[ \log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l)) \right]$$

è¿™æ˜¯ä¸€ä¸ª**äºŒåˆ†ç±»é—®é¢˜**ï¼šç»™å®š $(y_w, y_l)$ï¼Œé¢„æµ‹å“ªä¸ªæ›´å¥½ã€‚

Reward Model çš„æ¶æ„é€‰æ‹©ï¼š
- é€šå¸¸ç”¨ SFT æ¨¡å‹åˆå§‹åŒ–
- å»æ‰è¯­è¨€æ¨¡å‹å¤´ï¼ŒåŠ ä¸Šæ ‡é‡è¾“å‡ºå¤´
- è¾“å…¥ $(x, y)$ï¼Œè¾“å‡ºæ ‡é‡ $r_\phi(x, y) \in \mathbb{R}$

### Stage 3: PPO å¾®è°ƒ

ä½¿ç”¨ Reward Model æä¾›å¥–åŠ±ä¿¡å·ï¼Œç”¨ PPO ä¼˜åŒ–ç­–ç•¥ã€‚

> **RLHF ä¼˜åŒ–ç›®æ ‡**
>
> $$\max_\theta \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot|x)} \left[ r_\phi(x, y) \right] - \beta \cdot \text{KL}(\pi_\theta \| \pi_{\text{ref}})$$
>
> å…¶ä¸­ $\beta > 0$ æ˜¯ KL æ­£åˆ™ç³»æ•°ã€‚

#### KL æ­£åˆ™çš„ä½œç”¨

KL æ­£åˆ™é¡¹ $\text{KL}(\pi_\theta \| \pi_{\text{ref}})$ è‡³å…³é‡è¦ï¼š

1. **é˜²æ­¢ Reward Hacking**ï¼š
   - Reward Model æ˜¯ä¸å®Œç¾çš„ä»£ç†
   - æ— çº¦æŸä¼˜åŒ–ä¼šæ‰¾åˆ°"æ¬ºéª—" RM çš„æ–¹å¼
   - ä¾‹å¦‚ï¼šç”Ÿæˆç‰¹å®šæ¨¡å¼è·å¾—é«˜åˆ†ï¼Œä½†å®é™…è´¨é‡å·®

2. **ä¿æŒç”Ÿæˆè´¨é‡**ï¼š
   - SFT æ¨¡å‹å·²ç»æœ‰è¾ƒå¥½çš„è¯­è¨€èƒ½åŠ›
   - KL çº¦æŸé˜²æ­¢åç¦»å¤ªè¿œå¯¼è‡´æµåˆ©åº¦ä¸‹é™

3. **ç¨³å®šè®­ç»ƒ**ï¼š
   - çº¦æŸä¼˜åŒ–ç©ºé—´ï¼Œé¿å…ç­–ç•¥å´©æºƒ
   - æä¾›æ­£åˆ™åŒ–æ•ˆæœ

<div class="tikz-container">
<script type="text/tikz">
\begin{tikzpicture}[
    arrow/.style={->, thick, >=stealth}
]
    % åæ ‡è½´
    \draw[arrow] (-0.5, 0) -- (8, 0) node[right] {$\text{KL}(\pi_\theta \| \pi_{\text{ref}})$};
    \draw[arrow] (0, -0.5) -- (0, 5) node[above] {$\mathbb{E}[r_\phi]$};

    % æ›²çº¿
    \draw[thick, blue, domain=0.2:7, samples=100] plot (\x, {4 - 0.8*(\x-3)^2/9 + 0.5*ln(\x)});

    % æœ€ä¼˜ç‚¹
    \fill[red] (2.5, 3.8) circle (3pt);
    \node[font=\small, red] at (2.5, 4.3) {æœ€ä¼˜æƒè¡¡};

    % åŒºåŸŸæ ‡æ³¨
    \node[font=\scriptsize, align=center] at (1, 2) {KL å¤ªå°\\æ”¹è¿›æœ‰é™};
    \node[font=\scriptsize, align=center] at (6, 2) {KL å¤ªå¤§\\Reward Hacking};

    % beta çš„ä½œç”¨
    \draw[dashed, gray] (0, 3.8) -- (2.5, 3.8) -- (2.5, 0);
\end{tikzpicture}
</script>
</div>

#### PPO æ›´æ–°æµç¨‹

RLHF ä¸­ PPO çš„å…·ä½“æ­¥éª¤ï¼š

```
è¾“å…¥: SFT æ¨¡å‹ Ï€_refï¼ŒReward Model r_Ï†ï¼ŒKL ç³»æ•° Î²
åˆå§‹åŒ– Ï€_Î¸ â† Ï€_refï¼ŒCritic V_Ïˆ

for each iteration:
    // é‡‡æ ·
    ä» prompt åˆ†å¸ƒé‡‡æ · x âˆ¼ D
    ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆå›å¤ y âˆ¼ Ï€_Î¸(Â·|x)

    // è®¡ç®—å¥–åŠ±
    è®¡ç®— RM å¥–åŠ±ï¼šr^RM = r_Ï†(x, y)
    è®¡ç®— KL æƒ©ç½šï¼šr^KL_t = -Î² log [Ï€_Î¸(y_t|x, y_{<t}) / Ï€_ref(y_t|x, y_{<t})]
    æ€»å¥–åŠ±ï¼šr_t = r^KL_t + ğŸ™_{t=T} Â· r^RM

    // GAE è®¡ç®—
    ç”¨ Critic V_Ïˆ è®¡ç®— advantage Ã‚_t

    // PPO æ›´æ–°
    ç”¨ PPO-Clip ç›®æ ‡æ›´æ–° Ï€_Î¸
    ç”¨ TD ç›®æ ‡æ›´æ–° V_Ïˆ
```

> **é‡è¦æç¤º**ï¼šRLHF éœ€è¦ç»´æŠ¤çš„æ¨¡å‹ï¼š
> 1. $\pi_\theta$ï¼šæ­£åœ¨è®­ç»ƒçš„ç­–ç•¥ï¼ˆActive Modelï¼‰
> 2. $\pi_{\text{ref}}$ï¼šå‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“ï¼‰
> 3. $r_\phi$ï¼šReward Modelï¼ˆå†»ç»“ï¼‰
> 4. $V_\psi$ï¼šCritic ç½‘ç»œ
>
> å…± 4 ä¸ªå¤§æ¨¡å‹ï¼Œæ˜¾å­˜å¼€é”€å·¨å¤§ï¼è¿™æ˜¯ DPOã€GRPO ç­‰æ–¹æ³•è¯•å›¾è§£å†³çš„é—®é¢˜ã€‚

## Direct Preference Optimization (DPO)

DPO æ˜¯ä¸€ç§ç»•è¿‡ Reward Model å’Œ PPO çš„ç®€åŒ–æ–¹æ³•ï¼Œç”± Rafailov et al. 2023 æå‡ºã€‚

### DPO çš„åŠ¨æœº

RLHF + PPO çš„é—®é¢˜ï¼š
- **æ¨¡å‹å¼€é”€å¤§**ï¼šéœ€è¦ç»´æŠ¤ 4 ä¸ªæ¨¡å‹
- **é‡‡æ ·æˆæœ¬é«˜**ï¼šå¤§æ¨¡å‹åœ¨çº¿ç”Ÿæˆå¾ˆè´µ
- **å®ç°å¤æ‚**ï¼šPPO è¶…å‚æ•æ„Ÿï¼Œéœ€è¦ç²¾ç»†è°ƒå‚
- **è®­ç»ƒä¸ç¨³å®š**ï¼šRL è®­ç»ƒå®¹æ˜“å´©æºƒ

> **DPO çš„æ ¸å¿ƒé—®é¢˜**ï¼šèƒ½å¦ç›´æ¥åœ¨åå¥½æ•°æ® $(x, y_w, y_l)$ ä¸Šä¼˜åŒ–ï¼Œåƒç›‘ç£å­¦ä¹ ä¸€æ ·ç®€å•ï¼Ÿ

ç­”æ¡ˆæ˜¯å¯ä»¥çš„ï¼å…³é”®æ´å¯Ÿï¼šKL æ­£åˆ™çš„ RL é—®é¢˜æœ‰**é—­å¼è§£**ã€‚

### DPO Loss å…¬å¼

> **DPO Loss**
>
> $$L_{\text{DPO}}(\theta) = -\mathbb{E}_{(x, y_w, y_l)} \left[ \log \sigma \left( \beta \left[ \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right] \right) \right]$$

### DPO å®Œæ•´æ¨å¯¼

> **DPO ç­‰ä»·æ€§å®šç†**ï¼šDPO Loss ä¸ RLHF ç›®æ ‡åœ¨æœ€ä¼˜è§£å¤„ç­‰ä»·ã€‚

**è¯æ˜**ï¼šæ¨å¯¼åˆ†ä¸º 5 ä¸ªå…³é”®æ­¥éª¤ã€‚

**Step 1ï¼šRLHF ç›®æ ‡å±•å¼€**

RLHF ä¼˜åŒ–ç›®æ ‡ï¼š

$$\max_\pi \mathbb{E}_{y \sim \pi} \left[ r(x, y) \right] - \beta \cdot \text{KL}(\pi \| \pi_{\text{ref}})$$

å±•å¼€ KL æ•£åº¦ï¼š

$$= \mathbb{E}_{y \sim \pi} \left[ r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} \right]$$

**Step 2ï¼šå¼•å…¥é…åˆ†å‡½æ•° $Z(x)$**

ä¸ºäº†è®©æœ€ä¼˜ç­–ç•¥æ˜¯åˆæ³•çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå®šä¹‰é…åˆ†å‡½æ•°ï¼š

$$Z(x) = \sum_y \pi_{\text{ref}}(y|x) \exp\left( \frac{r(x,y)}{\beta} \right)$$

$Z(x)$ æ˜¯å½’ä¸€åŒ–å¸¸æ•°ï¼Œåªä¾èµ–äº $x$ï¼ˆä¸ä¾èµ–äºè¢«ä¼˜åŒ–çš„ç­–ç•¥ï¼‰ã€‚

**Step 3ï¼šæœ€ä¼˜ç­–ç•¥çš„é—­å¼è§£**

KL æ­£åˆ™ RL é—®é¢˜æœ‰é—­å¼è§£ï¼š

> **KL æ­£åˆ™ RL çš„æœ€ä¼˜ç­–ç•¥å¼•ç†**
>
> ç›®æ ‡ $\max_\pi \mathbb{E}_{y \sim \pi}[r(y)] - \beta \cdot \text{KL}(\pi \| \pi_{\text{ref}})$ çš„æœ€ä¼˜è§£ä¸ºï¼š
>
> $$\pi^*(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left( \frac{r(x,y)}{\beta} \right)$$

è¿™æ˜¯ä¸€ä¸ªæœ‰çº¦æŸä¼˜åŒ–é—®é¢˜ï¼ˆ$\pi$ éœ€è¦æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚ç›´è§‰ï¼šæœ€ä¼˜ç­–ç•¥æ˜¯å‚è€ƒç­–ç•¥æŒ‰ $\exp(r/\beta)$ é‡æ–°åŠ æƒã€‚å¥–åŠ±è¶Šé«˜ï¼Œæ¦‚ç‡æå‡è¶Šå¤šã€‚

**Step 4ï¼šä»æœ€ä¼˜ç­–ç•¥åè§£ reward**

å…³é”®æ­¥éª¤ï¼šä»æœ€ä¼˜ç­–ç•¥åè§£ rewardã€‚

å–å¯¹æ•°ï¼š

$$\log \pi^*(y|x) = \log \pi_{\text{ref}}(y|x) - \log Z(x) + \frac{r(x,y)}{\beta}$$

æ•´ç†å¾—ï¼š

$$r(x,y) = \beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)$$

> **æ ¸å¿ƒæ´å¯Ÿ**ï¼šreward å¯ä»¥ç”¨ç­–ç•¥çš„ log-ratio è¡¨ç¤ºï¼è™½ç„¶æœ‰ $\log Z(x)$ é¡¹ï¼Œä½†å®ƒåªä¾èµ–äº $x$ï¼Œåœ¨ pairwise æ¯”è¾ƒä¸­ä¼šæ¶ˆé™¤ã€‚

**Step 5ï¼šä»£å…¥ Bradley-Terry æ¨¡å‹ï¼Œ$Z(x)$ æ¶ˆé™¤**

å°† reward è¡¨è¾¾å¼ä»£å…¥ Bradley-Terry æ¨¡å‹ï¼š

$$\begin{align}
P(y_w \succ y_l) &= \sigma(r(x, y_w) - r(x, y_l)) \\
&= \sigma\left( \beta \left[ \log \frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi^*(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right] \right)
\end{align}$$

$\beta \log Z(x)$ é¡¹ç›¸æ¶ˆäº†ï¼

æœ€å¤§åŒ–åå¥½æ•°æ®çš„ log-likelihoodï¼Œç”¨ $\pi_\theta$ ä»£æ›¿ $\pi^*$ï¼Œå¾—åˆ° DPO Lossã€‚

<div class="tikz-container">
<script type="text/tikz">
\begin{tikzpicture}[
    box/.style={draw, rounded corners, fill=blue!10, minimum width=3.5cm, minimum height=1cm, align=center},
    arrow/.style={->, thick, >=stealth}
]
    \node[box] (rlhf) at (0, 3) {RLHF ç›®æ ‡\\$\max \mathbb{E}[r] - \beta \cdot \text{KL}$};
    \node[box] (opt) at (0, 1) {æœ€ä¼˜ç­–ç•¥é—­å¼è§£\\$\pi^* \propto \pi_{\text{ref}} \exp(r/\beta)$};
    \node[box] (reward) at (0, -1) {åè§£ reward\\$r = \beta \log \frac{\pi^*}{\pi_{\text{ref}}} + \beta \log Z$};
    \node[box, fill=green!20] (dpo) at (0, -3) {DPO Loss\\$Z(x)$ æ¶ˆé™¤};

    \draw[arrow] (rlhf) -- node[right, font=\small] {KL-RL é—­å¼è§£} (opt);
    \draw[arrow] (opt) -- node[right, font=\small] {å–å¯¹æ•°} (reward);
    \draw[arrow] (reward) -- node[right, font=\small] {ä»£å…¥ BT æ¨¡å‹} (dpo);
\end{tikzpicture}
</script>
</div>

> **DPO çš„æ ¸å¿ƒæ´å¯Ÿ**ï¼š
> 1. KL æ­£åˆ™ RL é—®é¢˜æœ‰é—­å¼è§£ï¼Œæœ€ä¼˜ç­–ç•¥æ˜¯å‚è€ƒç­–ç•¥çš„æŒ‡æ•°é‡åŠ æƒ
> 2. å¯ä»¥ä»æœ€ä¼˜ç­–ç•¥åè§£éšå¼ reward
> 3. é…åˆ†å‡½æ•° $Z(x)$ åœ¨ pairwise æ¯”è¾ƒä¸­æ¶ˆé™¤â€”â€”è¿™æ˜¯ DPO èƒ½ work çš„å…³é”®
> 4. æœ€ç»ˆå½¢å¼åªéœ€è¦è®¡ç®— log-probabilityï¼Œåƒç›‘ç£å­¦ä¹ ä¸€æ ·ç®€å•

### DPO çš„ç›´è§‚ç†è§£

å®šä¹‰**éšå¼å¥–åŠ±**ï¼š

$$\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}$$

DPO Loss å¯ä»¥å†™æˆï¼š

$$L_{\text{DPO}} = -\mathbb{E} \left[ \log \sigma(\hat{r}_\theta(x, y_w) - \hat{r}_\theta(x, y_l)) \right]$$

ç›´è§‰ï¼š
- $\hat{r}_\theta(x, y_w) > \hat{r}_\theta(x, y_l)$ï¼š$y_w$ çš„éšå¼å¥–åŠ±æ›´é«˜ï¼Œloss å˜å°
- è®­ç»ƒè¿‡ç¨‹æé«˜ $y_w$ ç›¸å¯¹äº $\pi_{\text{ref}}$ çš„æ¦‚ç‡ï¼Œå‹ä½ $y_l$ çš„æ¦‚ç‡
- $\beta$ æ§åˆ¶"ç›¸å¯¹äºå‚è€ƒç­–ç•¥åç¦»å¤šå°‘"çš„å°ºåº¦

### DPO vs RLHF å¯¹æ¯”

| ç‰¹æ€§ | RLHF + PPO | DPO |
|------|------------|-----|
| éœ€è¦ Reward Model | æ˜¯ | å¦ |
| éœ€è¦ Critic ç½‘ç»œ | æ˜¯ | å¦ |
| è®­ç»ƒæ–¹å¼ | åœ¨çº¿é‡‡æ · | ç¦»çº¿è®­ç»ƒ |
| æ¨¡å‹æ•°é‡ | 4 ä¸ª | 2 ä¸ª |
| å®ç°å¤æ‚åº¦ | é«˜ | ä½ |
| è¶…å‚æ•æ„Ÿæ€§ | é«˜ | ä½ |
| æ¢ç´¢èƒ½åŠ› | æœ‰ | æ—  |
| é€‚ç”¨åœºæ™¯ | å¤æ‚ä»»åŠ¡ | ç®€å•å¯¹é½ |

DPO çš„å±€é™ï¼š
- **æ— æ¢ç´¢**ï¼šå®Œå…¨ç¦»çº¿ï¼Œåªèƒ½åœ¨å·²æœ‰åå¥½æ•°æ®çš„åˆ†å¸ƒå†…ä¼˜åŒ–
- **Pairwise ä¿¡å·ç²—ç³™**ï¼šåªçŸ¥é“è°æ›´å¥½ï¼Œä¸çŸ¥é“å¥½å¤šå°‘
- **éš¾ä»»åŠ¡æå‡æœ‰é™**ï¼šåœ¨æ•°å­¦ã€ä»£ç ç­‰éœ€è¦æ¢ç´¢çš„ä»»åŠ¡ä¸Šæ•ˆæœä¸å¦‚ RL

## æœ¬ç« å°ç»“

1. **LLM å¯¹é½çš„ RL å»ºæ¨¡**ï¼šState = prompt + å·²ç”Ÿæˆ tokensï¼ŒAction = ä¸‹ä¸€ä¸ª tokenï¼Œç¨€ç–å¥–åŠ±åªåœ¨åºåˆ—ç»“æŸæ—¶ç»™å‡º

2. **RLHF ä¸‰é˜¶æ®µ**ï¼š
   - Stage 1 (SFT)ï¼šç›‘ç£å¾®è°ƒï¼Œå­¦ä¹ æŒ‡ä»¤éµå¾ª
   - Stage 2 (RM)ï¼šä»åå¥½æ•°æ®è®­ç»ƒ Reward Modelï¼ˆBradley-Terry æ¨¡å‹ï¼‰
   - Stage 3 (PPO)ï¼šç”¨ RM æä¾›å¥–åŠ±ï¼ŒPPO ä¼˜åŒ–ï¼ŒKL æ­£åˆ™é˜²æ­¢ reward hacking

3. **DPO**ï¼š
   - åˆ©ç”¨ KL-RL é—­å¼è§£ï¼Œç»•è¿‡ RM å’Œ PPO
   - ç›´æ¥åœ¨åå¥½æ•°æ®ä¸Šä¼˜åŒ–ï¼Œåƒç›‘ç£å­¦ä¹ ä¸€æ ·ç®€å•
   - åªéœ€ 2 ä¸ªæ¨¡å‹ï¼ˆ$\pi_\theta$ å’Œ $\pi_{\text{ref}}$ï¼‰
   - å±€é™ï¼šæ— æ¢ç´¢èƒ½åŠ›ï¼Œéš¾ä»»åŠ¡æå‡æœ‰é™

<div class="tikz-container">
<script type="text/tikz">
\begin{tikzpicture}[
    box/.style={draw, rounded corners, minimum width=3.5cm, minimum height=2cm, align=center},
    arrow/.style={->, thick, >=stealth}
]
    \node[box, fill=blue!20] (rlhf) at (0, 0) {
        \textbf{RLHF}\\[3pt]
        (2020-2022)\\[3pt]
        \scriptsize éœ€è¦ RM + Critic\\[-1pt]
        \scriptsize å®ç°å¤æ‚\\[-1pt]
        \scriptsize 4 ä¸ªæ¨¡å‹
    };

    \node[box, fill=green!20] (dpo) at (6, 0) {
        \textbf{DPO}\\[3pt]
        (2023)\\[3pt]
        \scriptsize ç¦»çº¿è®­ç»ƒ\\[-1pt]
        \scriptsize æ— æ¢ç´¢èƒ½åŠ›\\[-1pt]
        \scriptsize 2 ä¸ªæ¨¡å‹
    };

    \draw[arrow] (rlhf) -- node[above, font=\small] {ç®€åŒ–} (dpo);
\end{tikzpicture}
</script>
</div>

ä¸‹ä¸€ç¯‡å°†ä»‹ç» GRPOã€KL ä¼°è®¡å™¨ã€PRM ä»¥åŠ Long CoT RL ç­‰æ›´å…ˆè¿›çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•è¯•å›¾åœ¨ä¿æŒ DPO ç®€æ´æ€§çš„åŒæ—¶æ¢å¤åœ¨çº¿æ¢ç´¢èƒ½åŠ›ã€‚
