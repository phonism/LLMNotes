---
layout: post
title: "RL 学习笔记（一）：MDP 与价值函数"
date: 2025-12-19 08:00:00
author: Phonism
tags: [RL, MDP, Value Function]
lang: zh
translation: /rl-basics-en/
---

如何让机器学会下棋？监督学习的做法是给机器看大量棋谱，模仿人类高手的落子。但这有两个问题：一是棋谱覆盖不了所有可能的局面；二是下棋是**序列决策**——当前这步棋的好坏，可能要几十步后才能评判。

强化学习（Reinforcement Learning, RL）解决的正是这类问题：agent（智能体）通过与环境交互，在没有"标准答案"的情况下，逐渐学会做出最优决策。

本文的目标是建立 RL 的数学框架，核心结论是：RL 问题可以形式化为 Markov Decision Process（MDP），目标是找到使期望累积奖励最大的策略。

## 1. 强化学习概述

### 1.1 什么是强化学习

强化学习是机器学习的一个重要分支，研究 agent 如何通过与环境的交互来学习最优决策策略。与监督学习和无监督学习相比，RL 有三个显著特点：

- **交互式学习**：Agent 通过执行动作、观察反馈来积累经验，没有"正确答案"的标签
- **延迟奖励**：动作的好坏可能要很久之后才能知道（想想下棋时的"弃子争先"）
- **序列决策**：当前决策影响未来状态，数据分布由策略自身决定

```
┌─────────────────────────┐
│         Agent           │
│  ┌───────────────────┐  │
│  │   Policy π(a|s)   │  │
│  └───────────────────┘  │
└───────────┬─────────────┘
            │ Action aₜ
            ▼
┌─────────────────────────┐
│      Environment        │
└───────────┬─────────────┘
            │ State sₜ₊₁, Reward rₜ
            ▼
         (Loop)
```

**Agent-Environment 交互循环**：Agent 根据当前状态选择动作，环境根据动作返回奖励和新状态。这个循环不断重复，直到任务结束或无限持续。

### 1.2 与监督学习的本质区别

RL 与监督学习最关键的区别在于**数据分布**。监督学习假设数据独立同分布（i.i.d.），但 RL 中数据分布完全取决于当前策略——策略一变，采集到的数据就变了。

| 特性 | 监督学习 | 无监督学习 | 强化学习 |
|------|----------|------------|----------|
| 反馈类型 | 标签（正确答案） | 无反馈 | 奖励信号（标量） |
| 数据分布 | i.i.d. | i.i.d. | **由策略决定，非 i.i.d.** |
| 目标 | 最小化预测误差 | 发现数据结构 | 最大化累积奖励 |
| 决策时序 | 单步独立决策 | 无决策 | 序列相关决策 |

这个区别带来了实际训练中的诸多困难：策略变化导致数据分布变化，而数据分布变化又影响策略更新，形成复杂的动态过程。

> **RL 的三大核心挑战**：
> 1. **探索与利用的权衡**（Exploration vs. Exploitation）：应该尝试新动作（探索）还是使用已知好动作（利用）？过度探索浪费资源，过度利用可能错过更好的策略。
> 2. **信用分配问题**（Credit Assignment）：最终奖励如何归因到之前的各个动作？在下棋中，哪一步导致了最终的胜利或失败？
> 3. **非稳态性**：数据分布随策略变化而变化，这让训练变得不稳定，也是 RL 比监督学习更难的根本原因。

### 1.3 应用场景

RL 在众多领域取得了突破性进展：

- **游戏 AI**：AlphaGo（围棋）击败世界冠军、OpenAI Five（Dota 2）、AlphaStar（星际争霸）
- **机器人控制**：机械臂精细操作、四足机器人运动、无人机控制
- **自动驾驶**：决策规划、路径规划
- **推荐系统**：优化长期用户满意度而非即时点击率
- **大语言模型对齐**：RLHF（Reinforcement Learning from Human Feedback）已成为 GPT、Claude 等模型的标准训练流程

值得注意的是，近年来 RL 最成功的应用可能不是游戏，而是 LLM 对齐——这也是本系列后续文章的重点内容。

## 2. Markov Decision Process (MDP)

有了直观理解后，我们来建立数学框架。MDP 是 RL 的标准形式化工具，它把 agent 与环境的交互过程描述为一个离散时间随机控制过程。

### 2.1 MDP 五元组定义

**定义 (Markov Decision Process)**：一个 MDP 由五元组 $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ 定义：

- $\mathcal{S}$：**状态空间**（State Space），所有可能状态的集合
- $\mathcal{A}$：**动作空间**（Action Space），所有可能动作的集合
- $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0,1]$：**状态转移概率**，$P(s'\|s,a)$ 表示在状态 $s$ 执行动作 $a$ 后转移到状态 $s'$ 的概率
- $R: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$：**奖励函数**，$R(s,a)$ 表示在状态 $s$ 执行动作 $a$ 获得的即时奖励
- $\gamma \in [0,1]$：**折扣因子**（Discount Factor），权衡即时奖励与未来奖励的相对重要性

```
一个简单的 MDP 示例：

      a₁: P=0.7, r=1
    ┌────────────────► (s₂) ◄──┐ r=2
    │                          │
   (s₁)                        └──┘
    │
    │ a₁: P=0.3, r=1
    └────────────────► (s₃) ◄──┐ r=-1
                               │
    a₂: P=1, r=0               └──┘
    └──► (s₁) (原地)

状态 s₁ 有两个动作可选：
- a₁ 以 0.7 概率转移到 s₂（高奖励），0.3 概率转移到 s₃（负奖励）
- a₂ 原地不动但无奖励
```

> **注意**：奖励函数的定义有多种形式：$R(s,a)$、$R(s,a,s')$、$R(s)$。这些形式可以相互转化。例如，$R(s,a,s')$ 可以转化为 $R(s,a) = \sum_{s'} P(s'\|s,a) R(s,a,s')$。本文默认使用 $R(s,a)$ 形式。

### 2.2 Markov 性质

MDP 的核心假设是 **Markov 性质**（Markov Property），也称为"无记忆性"：

**定义 (Markov 性质)**：给定当前状态 $s_t$ 和动作 $a_t$，下一状态 $s_{t+1}$ 和奖励 $r_t$ 的分布只依赖于 $(s_t, a_t)$，与历史状态和动作无关：

$$P(s_{t+1}, r_t | s_t, a_t) = P(s_{t+1}, r_t | s_0, a_0, s_1, a_1, \ldots, s_t, a_t)$$

直观理解：**当前状态包含了预测未来所需的所有信息**。历史轨迹可以被"压缩"为当前状态，状态是对历史的"充分统计量"。

> **如果真实环境不满足 Markov 性质怎么办？** 答案是扩展状态表示：
> - **历史窗口**：Atari 游戏中用连续 4 帧图像作为状态，捕捉运动信息
> - **RNN 隐状态**：用循环神经网络的隐状态编码历史
> - **Transformer**：用 attention 机制处理变长历史
>
> 本质上是把"历史"编码进状态表示里，使得扩展后的状态满足 Markov 性质。

### 2.3 状态空间与动作空间的分类

根据状态空间和动作空间的性质，RL 问题可以分类为：

| 类型 | 状态空间 | 动作空间 | 典型例子 |
|------|----------|----------|----------|
| 离散-离散 | 有限/可数 | 有限/可数 | 棋类游戏（棋盘布局 × 落子位置） |
| 连续-离散 | $\mathbb{R}^n$ | 有限/可数 | Atari 游戏（像素图像 × 按键） |
| 连续-连续 | $\mathbb{R}^n$ | $\mathbb{R}^m$ | 机器人控制（关节角度 × 力矩） |

## 3. 轨迹与回报

### 3.1 轨迹的定义

Agent 与环境交互产生的状态-动作-奖励序列称为**轨迹**（Trajectory），也叫 episode 或 rollout。

**定义 (轨迹)**：轨迹 $\tau$ 是一个状态、动作、奖励的序列：

$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_{T-1}, a_{T-1}, r_{T-1}, s_T)$$

其中 $T$ 是轨迹的长度（episode 的终止时刻）。

```
轨迹的生成过程：

(s₀) ──π(a₀|s₀)──► [a₀] ──P,r₀──► (s₁) ──π(a₁|s₁)──► [a₁] ──P,r₁──► (s₂) ─...─► (sₜ)

○ = 状态    □ = 动作
策略 π 决定动作，环境 P 决定状态转移和奖励
```

### 3.2 轨迹概率分解

轨迹的概率如何计算？

**定理 (轨迹概率分解)**：在策略 $\pi$ 下，轨迹 $\tau$ 的概率为：

$$p(\tau | \pi) = p(s_0) \prod_{t=0}^{T-1} \pi(a_t | s_t) \cdot P(s_{t+1} | s_t, a_t)$$

**证明**：利用条件概率的链式法则和 Markov 性质：

$$\begin{aligned}
p(\tau | \pi) &= p(s_0, a_0, s_1, a_1, \ldots, s_T | \pi) \\
&= p(s_0) \cdot p(a_0 | s_0) \cdot p(s_1 | s_0, a_0) \cdot p(a_1 | s_1) \cdot p(s_2 | s_1, a_1) \cdots \\
&= p(s_0) \prod_{t=0}^{T-1} \underbrace{p(a_t | s_t)}_{\pi(a_t|s_t)} \cdot \underbrace{p(s_{t+1} | s_t, a_t)}_{P(s_{t+1}|s_t,a_t)}
\end{aligned}$$

其中第二步使用了 Markov 性质：$p(s_{t+1} \| s_0, a_0, \ldots, s_t, a_t) = p(s_{t+1} \| s_t, a_t)$。

> **这个分解非常重要！** 观察公式：
> - $p(s_0)$：初始状态分布，由环境决定
> - $\pi(a_t\|s_t)$：策略，是我们要学习/优化的对象
> - $P(s_{t+1}\|s_t,a_t)$：环境动力学，由环境决定
>
> **关键观察**：$p(s_0)$ 和 $P(s_{t+1}\|s_t,a_t)$ 与策略参数 $\theta$ 无关。因此，当对 $\log p(\tau\|\pi_\theta)$ 求关于 $\theta$ 的梯度时，环境动力学项会消失！这是 Policy Gradient 定理的核心。

### 3.3 Return（回报）的定义

**定义 (回报 / Reward-to-go)**：从时刻 $t$ 开始的**回报**（Return）或**Reward-to-go** 定义为未来奖励的折扣累积和：

$$G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k} = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots$$

其中 $\gamma \in [0,1]$ 是折扣因子。

回报 $G_t$ 满足一个简单但重要的递推关系：

$$G_t = r_t + \gamma G_{t+1}$$

Bellman 方程正是基于这个递推结构——下一篇文章将详细展开。

```
回报 Gₜ 的计算：

时间轴:  ──rₜ────rₜ₊₁────rₜ₊₂────rₜ₊₃────...──►
权重:     ×1     ×γ      ×γ²     ×γ³

Gₜ = rₜ + γ·rₜ₊₁ + γ²·rₜ₊₂ + γ³·rₜ₊₃ + ...

γ 越小，远期奖励的权重越低
```

### 3.4 Episodic vs Continuing Tasks

根据任务是否有终止状态，RL 问题分为两类：

- **Episodic Task**：存在终止状态 $s_T$，轨迹有限长度。例如：棋类游戏（胜负终局）、机器人完成特定任务、一局游戏
- **Continuing Task**：没有终止状态，$T \to \infty$。例如：股票交易、持续运行的控制系统、服务器调度

对于 Continuing Task，必须使用 $\gamma < 1$ 以确保回报有界。当 $|r_t| \leq R_{\max}$ 时：

$$|G_t| \leq \sum_{k=0}^{\infty} \gamma^k R_{\max} = \frac{R_{\max}}{1 - \gamma}$$

### 3.5 折扣因子的多重意义

折扣因子 $\gamma$ 是一个看似简单但含义丰富的超参数：

- $\gamma = 0$：Agent 只关心即时奖励，完全忽略未来（"短视"）
- $\gamma = 1$：Agent 平等对待所有未来奖励（仅适用于 Episodic Task）
- $0 < \gamma < 1$：权衡即时与未来奖励，$\gamma$ 越大越"远视"

> **折扣因子的多重理解**：
> 1. **数学角度**：确保无限和收敛，$G_t$ 有界
> 2. **经济学角度**：时间价值——"现在的 1 元比未来的 1 元更值钱"
> 3. **不确定性角度**：未来越远，预测越不准确，应降低权重
> 4. **实践角度**：$\gamma$ 定义了"有效时间尺度"。$1/(1-\gamma)$ 步之外的奖励贡献会指数衰减到可以忽略。例如 $\gamma=0.99$ 时，有效视野约 100 步。

## 4. 策略与价值函数

### 4.1 策略的定义

**定义 (策略)**：**策略**（Policy）$\pi$ 是从状态到动作的映射，定义了 agent 在每个状态下如何选择动作。策略分为两类：

- **确定性策略**（Deterministic Policy）：$\pi: \mathcal{S} \to \mathcal{A}$，即 $a = \pi(s)$
- **随机策略**（Stochastic Policy）：$\pi: \mathcal{S} \times \mathcal{A} \to [0,1]$，即 $\pi(a\|s)$ 表示在状态 $s$ 选择动作 $a$ 的概率

随机策略满足归一化条件：对于所有 $s \in \mathcal{S}$，

$$\sum_{a \in \mathcal{A}} \pi(a|s) = 1$$

为什么需要随机策略？确定性策略不是更简单吗？

> **随机策略的三个优势**：
> 1. **探索**：随机性有助于探索未知动作，避免陷入局部最优
> 2. **对抗性**：在博弈场景中，确定性策略容易被对手预测（想想"石头剪刀布"）
> 3. **优化友好**：随机策略的梯度更容易计算（确定性策略的梯度涉及 $\arg\max$，不可微）
>
> 确定性策略可以看作随机策略的特例（概率集中在单一动作上）。

### 4.2 状态价值函数 $V^\pi(s)$

**定义 (状态价值函数)**：策略 $\pi$ 的**状态价值函数**（State Value Function）定义为从状态 $s$ 出发，遵循策略 $\pi$ 的期望回报：

$$V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k} \mid S_t = s \right]$$

其中期望是对策略 $\pi$ 下产生的所有可能轨迹取的。

$V^\pi(s)$ 回答的问题是：**在状态 $s$，如果从现在开始一直遵循策略 $\pi$，能获得的期望总奖励是多少？**

### 4.3 动作价值函数 $Q^\pi(s,a)$

**定义 (动作价值函数)**：策略 $\pi$ 的**动作价值函数**（Action Value Function）定义为在状态 $s$ 执行动作 $a$，之后遵循策略 $\pi$ 的期望回报：

$$Q^\pi(s,a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k} \mid S_t = s, A_t = a \right]$$

$Q^\pi(s,a)$ 比 $V^\pi(s)$ 提供了更细粒度的信息：它允许我们单独评估每个动作的价值，而不是只知道"按 $\pi$ 走的整体期望"。这使得我们可以比较不同动作的好坏，从而改进策略。

### 4.4 $V$ 与 $Q$ 的关系

两个价值函数之间存在简单而重要的联系：

**定理 ($V$ 与 $Q$ 的关系)**：

$$V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^\pi(s,a) = \mathbb{E}_{a \sim \pi(\cdot|s)} \left[ Q^\pi(s,a) \right]$$

**证明**：由全期望公式：

$$\begin{aligned}
V^\pi(s) &= \mathbb{E}_\pi \left[ G_t \mid S_t = s \right] \\
&= \sum_{a \in \mathcal{A}} p(A_t = a | S_t = s) \cdot \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right] \\
&= \sum_{a \in \mathcal{A}} \pi(a|s) \cdot Q^\pi(s,a)
\end{aligned}$$

直观理解：状态价值 $V^\pi(s)$ 就是所有动作价值 $Q^\pi(s,a)$ 按策略概率 $\pi(a\|s)$ 加权的平均。

### 4.5 优势函数 $A^\pi(s,a)$

**定义 (优势函数)**：**优势函数**（Advantage Function）定义为动作价值与状态价值之差：

$$A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$$

优势函数衡量的是：在状态 $s$ 执行动作 $a$ 相对于"平均水平"（即按策略采样动作）好多少或差多少。

**定理 (优势函数的关键性质)**：优势函数在策略分布下的期望为零：

$$\mathbb{E}_{a \sim \pi(\cdot|s)} \left[ A^\pi(s,a) \right] = 0$$

**证明**：

$$\begin{aligned}
\mathbb{E}_{a \sim \pi(\cdot|s)} \left[ A^\pi(s,a) \right]
&= \mathbb{E}_{a \sim \pi} \left[ Q^\pi(s,a) - V^\pi(s) \right] \\
&= \mathbb{E}_{a \sim \pi} \left[ Q^\pi(s,a) \right] - V^\pi(s) \\
&= V^\pi(s) - V^\pi(s) = 0
\end{aligned}$$

> **优势函数的直观意义**：
> - $A(s,a) > 0$：动作 $a$ 优于平均水平
> - $A(s,a) < 0$：动作 $a$ 劣于平均水平
> - $A(s,a) = 0$：动作 $a$ 与平均水平持平
>
> 优势函数在 Policy Gradient 中的作用将在后续文章详细讨论。

### 4.6 最优策略与最优价值函数

**定义 (最优价值函数)**：**最优状态价值函数** $V^*(s)$ 和**最优动作价值函数** $Q^*(s,a)$ 定义为所有策略中能达到的最大价值：

$$\begin{aligned}
V^*(s) &= \max_\pi V^\pi(s) \\
Q^*(s,a) &= \max_\pi Q^\pi(s,a)
\end{aligned}$$

**定义 (最优策略)**：策略 $\pi^*$ 称为**最优策略**（Optimal Policy），如果对于所有状态 $s \in \mathcal{S}$：

$$V^{\pi^*}(s) = V^*(s)$$

**定理 (最优策略的存在性)**：对于有限 MDP（有限状态空间和动作空间），至少存在一个确定性最优策略。

最优策略可以由最优动作价值函数直接导出：

**定理 (从 $Q^*$ 导出最优策略)**：给定 $Q^*$，最优策略为：

$$\pi^*(s) = \arg\max_{a \in \mathcal{A}} Q^*(s,a)$$

这个结论是 Q-Learning 等 Value-Based 方法的理论基础：**只要学到 $Q^*$，就能直接得到最优策略**。

## 5. RL 问题的形式化目标

综合以上定义，强化学习的目标可以形式化为：

**定义 (RL 目标)**：找到最优策略 $\pi^*$，使得期望回报最大化：

$$\pi^* = \arg\max_\pi J(\pi)$$

其中 $J(\pi)$ 是策略的性能指标，常见定义包括：

1. **从固定初始状态出发**：$J(\pi) = V^\pi(s_0)$
2. **初始状态有分布**：$J(\pi) = \mathbb{E}_{s_0 \sim p(s_0)} \left[ V^\pi(s_0) \right]$
3. **轨迹期望**：$J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]$

这三种定义在大多数情况下是等价的。后续文章将介绍解决这一优化问题的不同方法：

- **Value-Based 方法**：学习 $V^*$ 或 $Q^*$，再由 $\arg\max$ 导出 $\pi^*$
- **Policy-Based 方法**：直接优化参数化策略 $\pi_\theta$
- **Actor-Critic 方法**：同时学习策略（Actor）和价值函数（Critic）

```
         ┌─────────────────┐
         │   RL Methods    │
         └────────┬────────┘
                  │
    ┌─────────────┼─────────────┐
    ▼             ▼             ▼
┌────────┐  ┌────────┐  ┌────────────┐
│Value-  │  │Policy- │  │Actor-      │
│Based   │  │Based   │  │Critic      │
│Learn Q*│  │Learn πθ│  │Learn both  │
└────────┘  └────────┘  └────────────┘
    │             │             │
DQN          REINFORCE      A2C, PPO
Q-Learning   TRPO           SAC
```

## 本章小结

回顾开篇问题：如何让机器学会序列决策？答案是将问题形式化为 MDP，目标是最大化期望累积奖励。

**本章建立的核心概念**：

1. **MDP 五元组** $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ 是 RL 的标准形式化框架
2. **Markov 性质**：当前状态包含预测未来所需的所有信息
3. **轨迹概率分解**：$p(\tau\|\pi) = p(s_0) \prod_t \pi(a_t\|s_t) P(s_{t+1}\|s_t,a_t)$，策略与环境动力学分离
4. **回报的递推**：$G_t = r_t + \gamma G_{t+1}$，这是 Bellman 方程的起点
5. **价值函数**：$V^\pi$ 和 $Q^\pi$ 衡量策略的长期价值
6. **优势函数**：$A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$，期望为零
7. **RL 目标**：$\max_\pi J(\pi)$，找到使期望回报最大的策略

下一篇文章将介绍 Bellman 方程——利用回报的递推结构来高效计算价值函数，以及基于价值函数的 Q-Learning、DQN 等算法。
